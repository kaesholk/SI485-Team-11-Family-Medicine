{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install and import required libraries\n",
    "\n",
    "%pip install pandas ftfy keybert scikit-learn thefuzz\n",
    "\n",
    "import pandas as pd\n",
    "import ftfy\n",
    "import re\n",
    "import unicodedata\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from thefuzz import process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_format_data(pubmed_path, scopus_path, scholar_path, keywords_path):\n",
    "    \"\"\"\n",
    "    Loads and formats publication data from PubMed, Scopus, and Google Scholar,\n",
    "    and loads keyword data generated from an NLP algorithim.\n",
    "\n",
    "    Args:\n",
    "        pubmed_path (str): Path to the PubMed CSV file.\n",
    "        scopus_path (str): Path to the Scopus CSV file.\n",
    "        scholar_path (str): Path to the Google Scholar CSV file.\n",
    "        keywords_path (str): Path to the keywords CSV file.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (pubmed_df, scopus_df, scholar_df, keywords_df) — formatted DataFrames.\n",
    "    \"\"\"\n",
    "    # Load data from CSVs\n",
    "    pubmed = pd.read_csv(pubmed_path)\n",
    "    scopus = pd.read_csv(scopus_path)\n",
    "    google_scholar = pd.read_csv(scholar_path)\n",
    "    our_keywords = pd.read_csv(keywords_path)\n",
    "\n",
    "    # Standardize column names\n",
    "    pubmed.rename(columns={\"Full Name\": \"Name\"}, inplace=True)\n",
    "    google_scholar.rename(columns={\"Venue\": \"Journal\", \"keywords\": \"Keywords\"}, inplace=True)\n",
    "\n",
    "    # Clean Google Scholar keywords\n",
    "    def format_gs_keywords(keywords_list):\n",
    "        return str(keywords_list).replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\").replace(\",\", \";\")\n",
    "\n",
    "    google_scholar['Keywords'] = google_scholar['Keywords'].apply(lambda x: format_gs_keywords(x))\n",
    "\n",
    "    return pubmed, scopus, google_scholar, our_keywords\n",
    "\n",
    "def clean_article_title(title):\n",
    "    \"\"\"\n",
    "    Fixes encoding issues and special character artifacts in article titles.\n",
    "    \"\"\"\n",
    "    title = ftfy.fix_text(title)\n",
    "    replacements = {\n",
    "        \"Œ±\": \"α\", \"Œ≤\": \"β\", \"Œº\": \"μ\", \"Œî\": \"Δ\",\n",
    "        \"‚Äò\": \"'\", \"‚Äù\": '\"', \"‚Äú\": '\"', \"‚Äì\": \"-\", \"‚Äô\": \"'\"\n",
    "    }\n",
    "    for bad, good in replacements.items():\n",
    "        title = title.replace(bad, good)\n",
    "    return title.rstrip('.')\n",
    "\n",
    "def clean_name(name):\n",
    "    \"\"\"\n",
    "    Cleans and standardizes author names by normalizing unicode and formatting.\n",
    "    \"\"\"\n",
    "    name = unicodedata.normalize(\"NFKD\", name).encode(\"ASCII\", \"ignore\").decode(\"utf-8\")\n",
    "    name = re.sub(r'\\s+', ' ', name).strip()\n",
    "    if ',' in name:\n",
    "        name = name.split(',', 1)[0]\n",
    "    name = name.title()\n",
    "    name = re.sub(r'\\b([A-Z]) (?=[A-Z][a-z])', r'\\1. ', name)\n",
    "    return name\n",
    "\n",
    "def normalize_name(name):\n",
    "    \"\"\"\n",
    "    Reduces name to first and last names only for standardization.\n",
    "    \"\"\"\n",
    "    name_parts = name.split()\n",
    "    if len(name_parts) > 2:\n",
    "        name = \" \".join([name_parts[0], name_parts[-1]])\n",
    "    return name\n",
    "\n",
    "def standardize_name(name, name_list):\n",
    "    \"\"\"\n",
    "    Finds the closest match for a name in a provided name list using fuzzy matching.\n",
    "    \"\"\"\n",
    "    name = normalize_name(name)\n",
    "    match = process.extractOne(name, name_list, scorer=fuzz.ratio)\n",
    "    return match[0]\n",
    "\n",
    "def merge_and_clean_data(pubmed, scopus, google_scholar):\n",
    "    \"\"\"\n",
    "    Merges and consolidates data from PubMed, Scopus, and Google Scholar,\n",
    "    filling in missing data and standardizing columns.\n",
    "    \"\"\"\n",
    "    merged_df = pd.merge(pubmed, scopus, how=\"outer\", on=\"Title\", suffixes=('_pubmed', '_scopus'))\n",
    "    merged_df = merged_df.merge(google_scholar, on=\"Title\", how=\"outer\")\n",
    "\n",
    "    for col in google_scholar.columns:\n",
    "        if col != \"Title\":\n",
    "            merged_df.rename(columns={col: f\"{col}_google_scholar\"}, inplace=True)\n",
    "\n",
    "    # Fill missing PubMed fields using Scopus and then Google Scholar\n",
    "    for pubmed_column in merged_df.columns[merged_df.columns.str.contains('pubmed')]:\n",
    "        scopus_column = pubmed_column.replace(\"pubmed\", \"scopus\")\n",
    "        if scopus_column in merged_df.columns:\n",
    "            merged_df[pubmed_column] = merged_df[pubmed_column].combine_first(merged_df[scopus_column])\n",
    "\n",
    "    for pubmed_column in merged_df.columns[merged_df.columns.str.contains('pubmed')][:-1]:\n",
    "        gs_column = pubmed_column.replace(\"pubmed\", \"google_scholar\")\n",
    "        if gs_column in merged_df.columns:\n",
    "            merged_df[pubmed_column] = merged_df[pubmed_column].combine_first(merged_df[gs_column])\n",
    "\n",
    "    merged_df.rename(columns={\n",
    "        \"Name_pubmed\": \"Name\",\n",
    "        \"DOI_pubmed\": \"DOI\",\n",
    "        \"Year_pubmed\": \"Year\",\n",
    "        \"Journal_pubmed\": \"Journal\",\n",
    "        \"Scopus ID_scopus\": \"Scopus ID\",\n",
    "        \"ORCID_scopus\": \"ORCID\",\n",
    "        \"Author ID_google_scholar\": \"Google Scholar ID\",\n",
    "    }, inplace=True)\n",
    "\n",
    "    merged_df.fillna({\"Year\": 0}, inplace=True)\n",
    "    merged_df.fillna(\"\", inplace=True)\n",
    "    merged_df[\"Year\"] = merged_df[\"Year\"].astype(int)\n",
    "    merged_df[\"DOI\"] = merged_df.apply(lambda row: \"\" if \"DOI not found\" in row[\"DOI\"] else row[\"DOI\"], axis=1)\n",
    "\n",
    "    # Combine keywords from all sources\n",
    "    merged_df[\"Keywords\"] = merged_df.apply(lambda row: \"; \".join(filter(None, [\n",
    "        row.get(\"Keywords_pubmed\", \"\"), row.get(\"Keywords_scopus\", \"\")]\n",
    "    )), axis=1)\n",
    "    merged_df[\"Keywords\"] = merged_df.apply(lambda row: \"; \".join(filter(None, [\n",
    "        row.get(\"Keywords\", \"\"), row.get(\"Keywords_google_scholar\", \"\")]\n",
    "    )), axis=1)\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "def finalize_dataset(merged_df):\n",
    "    \"\"\"\n",
    "    Final cleaning steps including deduplication, name standardization, and saving.\n",
    "    \"\"\"\n",
    "    # Select and clean columns\n",
    "    final_df = merged_df[[\n",
    "        \"Name\", \"Title\", \"Year\", \"Journal\", \"Type\",\n",
    "        \"Keywords\", \"Abstract\", \"DOI\", \"Article Affiliation\",\n",
    "        \"ORCID\", \"Scopus ID\", \"Google Scholar ID\"\n",
    "    ]]\n",
    "    final_df = final_df[final_df[\"Year\"] != 0]\n",
    "\n",
    "    final_df[\"Title\"] = final_df[\"Title\"].apply(clean_article_title)\n",
    "    final_df[\"Name\"] = final_df[\"Name\"].apply(clean_name)\n",
    "    final_df = final_df.drop_duplicates(subset=[\"Title\", \"Name\"])\n",
    "    final_df = final_df[(final_df[\"Scopus ID\"] != \"\") | (final_df[\"ORCID\"] != \"\")]\n",
    "\n",
    "    # Standardize names\n",
    "    name_list = final_df['Name'].apply(normalize_name).unique().tolist()\n",
    "    final_df['standardized_name'] = final_df['Name'].apply(lambda x: standardize_name(x, name_list))\n",
    "\n",
    "    return final_df\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to orchestrate data loading, merging, cleaning, and exporting.\n",
    "    \"\"\"\n",
    "    # Load CSVs\n",
    "    pubmed_path = '/combine orcid scopus final.csv'\n",
    "    scopus_path = '/scopus_publications_2:4.csv'\n",
    "    scholar_path = '/google_scholar_updated.csv'\n",
    "    keywords_path = '/keyword_updated_projects.csv'\n",
    "\n",
    "    pubmed, scopus, google_scholar, our_keywords = load_and_format_data(\n",
    "    pubmed_path, scopus_path, scholar_path, keywords_path\n",
    ")\n",
    "\n",
    "    # Merge and clean\n",
    "    merged_df = merge_and_clean_data(pubmed, scopus, google_scholar)\n",
    "\n",
    "    # Finalize dataset\n",
    "    final_df = finalize_dataset(merged_df)\n",
    "\n",
    "    # Export\n",
    "    final_df.to_csv('final_dataset.csv', index=False)\n",
    "    print(\"Dataset saved to final_dataset.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
